{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 36218,
     "status": "ok",
     "timestamp": 1749800138282,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "spUfm5L2NbHR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --upgrade --quiet langchain langchain-community langchainhub langchain-chroma beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749800479914,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "7pTcoqFeNjiT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG_With_Memory\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCKvWXXEOqUzo3u8WxoHG99fAdyIE4ZHOU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1749800480303,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "k6vrHz96N1c7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749800480501,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "YcXVeDmUN5Dc"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1749800507240,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "ScxwpI-NN7D5"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",convert_system_message_to_human=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20710,
     "status": "ok",
     "timestamp": 1749800528199,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "ztPntYoBOGU7",
    "outputId": "48533127-26cf-49f2-b5ba-d96d12b0d295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"hi\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1749800551613,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "44MD7Rh-OLmU"
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1749800568783,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "l56wfeS4PYIH"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1749800687650,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "51zYLN23PcZk"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 456,
     "status": "ok",
     "timestamp": 1749800698599,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "vmw2yGc3P5Y8"
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1749800703058,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "3s4283oqP7-v",
    "outputId": "b10f540c-4c77-45e7-840c-e4ac4f984ac5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1749800723083,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "cqa39mZ-P9Lg"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1749800756156,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "LVl_G_S8QCEO"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1749801579358,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "djlh1AFcRHet"
   },
   "outputs": [],
   "source": [
    "from bs4 import SoupStrainer\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# Load webpage content\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(parse_only=SoupStrainer(class_=(\"post-content\", \"post-title\",\"post-headers\")))\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "doc = loader.load()\n",
    "\n",
    "# Split the document into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(doc)\n",
    "\n",
    "# Set up Gemini embeddings (replace API key with your actual key safely)\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\",\n",
    "    google_api_key=\"AIzaSyCKvWXXEOqUzo3u8WxoHG99fAdyIE4ZHOU\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1749801580856,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "zuXvV3ueSfHC"
   },
   "outputs": [],
   "source": [
    "vectorstore= Chroma.from_documents(documents=splits,embedding=gemini_embeddings)\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1749801620511,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "T9dSMdokS2WC",
    "outputId": "54193a1f-151b-4776-ab62-3786cc0b64fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x0000028012C2E270>, search_kwargs={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1749801753448,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "Z431HRHBTdI-"
   },
   "outputs": [],
   "source": [
    "system_prompt=(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces of retrieved context to answer the questions.\"\n",
    "    \"If you don't know the answer, say that you don't know.\"\n",
    "    \"Use three sentences maximum and keep the answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1749801831419,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "5_o9BOy-T9oJ"
   },
   "outputs": [],
   "source": [
    "chat_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1749801948623,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "Wpjha3PSUQqF"
   },
   "outputs": [],
   "source": [
    "question_answering_chain= create_stuff_documents_chain(model,chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1749801957130,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "APE2L4t9UtR3"
   },
   "outputs": [],
   "source": [
    "rag_chain=create_retrieval_chain(retriever,question_answering_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 10332,
     "status": "ok",
     "timestamp": 1749801997647,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "hnRRVaY2UvW4"
   },
   "outputs": [],
   "source": [
    "response=rag_chain.invoke({\"input\":\"What is MRKL?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1749801998537,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "HMiKMwljU2t0",
    "outputId": "68bc711b-7343-457f-cb39-de280667055a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MRKL, short for \"Modular Reasoning, Knowledge and Language,\" is a neuro-symbolic architecture for autonomous agents.  It uses an LLM to route inquiries to the most appropriate \"expert\" module (neural or symbolic).  These modules handle tasks the LLM itself can\\'t perform reliably.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1749802045157,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "_2zOc1cEU5da"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1749805557564,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "Hue-cOy4U_jw"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1749805671573,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "2YQ6R2iFieVg"
   },
   "outputs": [],
   "source": [
    "retriever_prompt=(\n",
    "    \"Given a chat history and the latest user question which might referenece context in the chat history,\"\n",
    "    \"formulate a standalone question which can be understood without the chat history.\"\n",
    "    \"Do not answer the question, just reformulate it if needed and otherwise return it as it is.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1749805977582,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "ij9t_hrii6Le"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", retriever_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1749805978201,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "sIIoVL4GjS5J"
   },
   "outputs": [],
   "source": [
    "history_aware_retriever=create_history_aware_retriever(model,retriever,contextualize_q_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1749805994021,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "swWACuVWjlhl"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "ok",
     "timestamp": 1749806044016,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "HRFzUKIvkI52"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1749806241716,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "AoGDtGV7kNza"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Use the provided context to answer the user's question.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"Context:\\n{context}\\n\\nQuestion:\\n{input}\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1749806298613,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "LC1lSzbrlGqS"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(\n",
    "    llm=model,\n",
    "    prompt=qa_prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1749806300190,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "GvKy4nUSkroi"
   },
   "outputs": [],
   "source": [
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1749806320428,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "-wLk1-XvlQMS"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1749806328305,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "d0jwHYfLlV0S"
   },
   "outputs": [],
   "source": [
    "chat_history=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1749806336848,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "n3L7lOfilabH"
   },
   "outputs": [],
   "source": [
    "question1 = \"What is Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 2556,
     "status": "ok",
     "timestamp": 1749806344251,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "SXWuoJETlcmk"
   },
   "outputs": [],
   "source": [
    "message1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1749806348669,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "W3W6eaIfldxz",
    "outputId": "6ad585f1-0379-42ac-d680-429f3fe71a7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a method used to break down complex tasks into smaller, simpler sub-tasks.  This makes the overall task easier to manage and solve.  Several techniques exist, including:\\n\\n* **Chain of Thought (CoT):**  This prompts the model to think step-by-step, explicitly decomposing the problem.\\n\\n* **Tree of Thoughts (ToT):**  This extends CoT by exploring multiple reasoning paths at each step, creating a tree of possible solutions.  It can use breadth-first search (BFS) or depth-first search (DFS) to explore this tree.\\n\\n* **LLM-based decomposition:**  This can be achieved with simple prompts asking for steps or subgoals, using task-specific instructions, or with human input.\\n\\n* **LLM+P:** This outsources the planning to an external classical planner using the Planning Domain Definition Language (PDDL).  The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message1[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1749806380049,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "4PTnB7mRlfeV"
   },
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question1),\n",
    "        AIMessage(content=message1[\"answer\"])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2065,
     "status": "ok",
     "timestamp": 1749806504791,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "a5y9U55VlnJb"
   },
   "outputs": [],
   "source": [
    "question2=\"What are the common ways of doing it?\"\n",
    "message2=rag_chain.invoke({\"input\":question2,\"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1749806509915,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "6TIw-ALMmFGf",
    "outputId": "37dce5fd-d314-45f5-ae2e-3ba1c9e02d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, common ways of performing task decomposition include:\n",
      "\n",
      "1. **Using Chain of Thought (CoT):**  Prompting the model to think step-by-step to break down complex tasks into smaller, simpler ones.\n",
      "\n",
      "2. **Using Tree of Thoughts (ToT):**  Extending CoT by exploring multiple reasoning paths simultaneously, creating a tree of potential solutions.\n",
      "\n",
      "3. **LLM-based decomposition with simple prompts:**  Using straightforward prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\n",
      "\n",
      "4. **Using task-specific instructions:** Giving instructions tailored to the task, such as \"Write a story outline\" for writing a novel.\n",
      "\n",
      "5. **Using human input:**  Directly involving human input to decompose the task.\n",
      "\n",
      "6. **LLM+P:** Outsourcing the planning to an external classical planner using PDDL as an intermediary language.  The LLM translates the problem and the plan into and from PDDL.\n"
     ]
    }
   ],
   "source": [
    "print(message2[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1749806597446,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "ytaGQeZgmG1w"
   },
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1749806671837,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "M8mSynMxmcNx"
   },
   "outputs": [],
   "source": [
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "executionInfo": {
     "elapsed": 2677,
     "status": "ok",
     "timestamp": 1749806675695,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "sc2xNUNJmuYi",
    "outputId": "8f7c1f64-f2ac-4041-94de-fc5b863af0bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller, simpler sub-tasks.  This makes the overall task easier to manage and solve.  Several methods exist, including:\\n\\n* **Chain of Thought (CoT):**  This method guides the model to \"think step by step,\" using more computational resources at test time to decompose the task.  It transforms a large task into multiple smaller, manageable ones, offering insight into the model\\'s reasoning process.\\n\\n* **Tree of Thoughts (ToT):**  This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of potential solutions.  Search methods like breadth-first search (BFS) or depth-first search (DFS) are used, with each state evaluated by a classifier or majority vote.\\n\\n* **LLM prompting:**  Simple prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" can elicit task decomposition from an LLM.  Task-specific instructions, such as \"Write a story outline,\" can also be used.  Human input can also guide the decomposition process.\\n\\n* **LLM+P:** This approach uses an external classical planner (often utilizing PDDL, the Planning Domain Definition Language) to handle long-horizon planning.  The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.  This outsources the planning to an external tool.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }, # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1749806680008,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "bmArRxcgmurU",
    "outputId": "65c721ab-5469-4310-a3b1-d0e1b6db2fce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?', additional_kwargs={}, response_metadata={}), AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller, simpler sub-tasks.  This makes the overall task easier to manage and solve.  Several methods exist, including:\\n\\n* **Chain of Thought (CoT):**  This method guides the model to \"think step by step,\" using more computational resources at test time to decompose the task.  It transforms a large task into multiple smaller, manageable ones, offering insight into the model\\'s reasoning process.\\n\\n* **Tree of Thoughts (ToT):**  This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of potential solutions.  Search methods like breadth-first search (BFS) or depth-first search (DFS) are used, with each state evaluated by a classifier or majority vote.\\n\\n* **LLM prompting:**  Simple prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" can elicit task decomposition from an LLM.  Task-specific instructions, such as \"Write a story outline,\" can also be used.  Human input can also guide the decomposition process.\\n\\n* **LLM+P:** This approach uses an external classical planner (often utilizing PDDL, the Planning Domain Definition Language) to handle long-horizon planning.  The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.  This outsources the planning to an external tool.', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1749806736965,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "cvCvz5LvmwXf",
    "outputId": "af6ef59e-fd85-4329-d9c7-bf20b7033038"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided text, common ways of performing task decomposition include:\\n\\n1. **Chain of Thought (CoT):**  Instructing the model to think step-by-step to break down complex tasks into smaller, simpler ones.\\n\\n2. **Tree of Thoughts (ToT):**  Extending CoT by exploring multiple reasoning paths simultaneously, creating a tree of potential solutions.\\n\\n3. **LLM prompting:** Using simple prompts (e.g., \"Steps for XYZ. 1.\", \"What are the subgoals for achieving XYZ?\") or task-specific instructions (e.g., \"Write a story outline\") to guide the LLM to decompose the task.  Human input can also be used in this process.\\n\\n4. **LLM+P:**  Utilizing an external classical planner (often using PDDL) to handle long-horizon planning. The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1749806740474,
     "user": {
      "displayName": "Akhil Gupta",
      "userId": "16103386141137431298"
     },
     "user_tz": -330
    },
    "id": "RSA1TaXPm5ux",
    "outputId": "633a7772-d063-40e8-d69d-5a038422b282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What is Task Decomposition?\n",
      "\n",
      "AI: Task decomposition is a technique used to break down complex tasks into smaller, simpler sub-tasks.  This makes the overall task easier to manage and solve.  Several methods exist, including:\n",
      "\n",
      "* **Chain of Thought (CoT):**  This method guides the model to \"think step by step,\" using more computational resources at test time to decompose the task.  It transforms a large task into multiple smaller, manageable ones, offering insight into the model's reasoning process.\n",
      "\n",
      "* **Tree of Thoughts (ToT):**  This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of potential solutions.  Search methods like breadth-first search (BFS) or depth-first search (DFS) are used, with each state evaluated by a classifier or majority vote.\n",
      "\n",
      "* **LLM prompting:**  Simple prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" can elicit task decomposition from an LLM.  Task-specific instructions, such as \"Write a story outline,\" can also be used.  Human input can also guide the decomposition process.\n",
      "\n",
      "* **LLM+P:** This approach uses an external classical planner (often utilizing PDDL, the Planning Domain Definition Language) to handle long-horizon planning.  The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.  This outsources the planning to an external tool.\n",
      "\n",
      "User: What are common ways of doing it?\n",
      "\n",
      "AI: Based on the provided text, common ways of performing task decomposition include:\n",
      "\n",
      "1. **Chain of Thought (CoT):**  Instructing the model to think step-by-step to break down complex tasks into smaller, simpler ones.\n",
      "\n",
      "2. **Tree of Thoughts (ToT):**  Extending CoT by exploring multiple reasoning paths simultaneously, creating a tree of potential solutions.\n",
      "\n",
      "3. **LLM prompting:** Using simple prompts (e.g., \"Steps for XYZ. 1.\", \"What are the subgoals for achieving XYZ?\") or task-specific instructions (e.g., \"Write a story outline\") to guide the LLM to decompose the task.  Human input can also be used in this process.\n",
      "\n",
      "4. **LLM+P:**  Utilizing an external classical planner (often using PDDL) to handle long-horizon planning. The LLM translates the problem into PDDL, the planner generates a plan, and the LLM translates the plan back into natural language.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"abc123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"AI\"\n",
    "    else:\n",
    "        prefix = \"User\"\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMgI0IPVAGJD887XkgthLsW",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
